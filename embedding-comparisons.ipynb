{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U lightgbm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:36:57.923858Z","iopub.execute_input":"2025-01-02T14:36:57.924145Z","iopub.status.idle":"2025-01-02T14:37:02.414055Z","shell.execute_reply.started":"2025-01-02T14:36:57.924122Z","shell.execute_reply":"2025-01-02T14:37:02.413055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport ast\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgbm\nimport catboost as cb\nimport xgboost as xgb \n# from sentence_transformers import SentenceTransformer\nfrom ast import literal_eval\nimport re\nfrom datetime import datetime\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:02.415158Z","iopub.execute_input":"2025-01-02T14:37:02.415398Z","iopub.status.idle":"2025-01-02T14:37:21.016864Z","shell.execute_reply.started":"2025-01-02T14:37:02.415378Z","shell.execute_reply":"2025-01-02T14:37:21.016158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datetime import datetime\n\ndef get_formatted_time_date():\n    now = datetime.now()\n    hour = now.strftime(\"%I\").lstrip('0')  # Remove leading zero from hour\n    minute = now.strftime(\"%M\")\n    am_pm = now.strftime(\"%p\").lower()\n    day = now.strftime(\"%d\").lstrip('0')  # Remove leading zero from day\n    month = now.strftime(\"%b\").lower()  # Abbreviated month name in lowercase\n    year = now.strftime(\"%Y\")\n    return f\"{hour}-{minute}-{am_pm}-{day}-{month}-{year}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:21.018538Z","iopub.execute_input":"2025-01-02T14:37:21.019207Z","iopub.status.idle":"2025-01-02T14:37:21.023517Z","shell.execute_reply.started":"2025-01-02T14:37:21.019184Z","shell.execute_reply":"2025-01-02T14:37:21.022811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\ntest = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:21.024982Z","iopub.execute_input":"2025-01-02T14:37:21.025188Z","iopub.status.idle":"2025-01-02T14:37:21.354004Z","shell.execute_reply.started":"2025-01-02T14:37:21.025170Z","shell.execute_reply":"2025-01-02T14:37:21.353027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = train.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:21.354883Z","iopub.execute_input":"2025-01-02T14:37:21.355199Z","iopub.status.idle":"2025-01-02T14:37:21.360170Z","shell.execute_reply.started":"2025-01-02T14:37:21.355166Z","shell.execute_reply":"2025-01-02T14:37:21.359555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_values = df.isnull().sum()\nmissing_percentages = (missing_values / len(df)) * 100\nmissing_info = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_percentages\n})\nmissing_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:21.361035Z","iopub.execute_input":"2025-01-02T14:37:21.361330Z","iopub.status.idle":"2025-01-02T14:37:21.398741Z","shell.execute_reply.started":"2025-01-02T14:37:21.361309Z","shell.execute_reply":"2025-01-02T14:37:21.397829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot missing values and percentages\nplt.figure(figsize=(12, 15))\n\n# Bar plot for missing percentages\nplt.subplot(2,1, 2)\nmissing_info['Percentage'].plot(kind='bar', color='salmon')\nplt.title('Missing Percentages')\nplt.xlabel('Columns')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=90)\n\nplt.tight_layout()\nplt.savefig('missing.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:21.399534Z","iopub.execute_input":"2025-01-02T14:37:21.399821Z","iopub.status.idle":"2025-01-02T14:37:22.212461Z","shell.execute_reply.started":"2025-01-02T14:37:21.399801Z","shell.execute_reply":"2025-01-02T14:37:22.211576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Create a word cloud for the 'skills' column\nskills_text = ' '.join(df['skills'].dropna().astype(str))\n\n# Generate the word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(skills_text)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud for Skills')\nplt.tight_layout()\nplt.savefig('wordcloud.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:22.214574Z","iopub.execute_input":"2025-01-02T14:37:22.214847Z","iopub.status.idle":"2025-01-02T14:37:24.164493Z","shell.execute_reply.started":"2025-01-02T14:37:22.214825Z","shell.execute_reply":"2025-01-02T14:37:24.163503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.165815Z","iopub.execute_input":"2025-01-02T14:37:24.166440Z","iopub.status.idle":"2025-01-02T14:37:24.190572Z","shell.execute_reply.started":"2025-01-02T14:37:24.166404Z","shell.execute_reply":"2025-01-02T14:37:24.189964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.191262Z","iopub.execute_input":"2025-01-02T14:37:24.191448Z","iopub.status.idle":"2025-01-02T14:37:24.194769Z","shell.execute_reply.started":"2025-01-02T14:37:24.191432Z","shell.execute_reply":"2025-01-02T14:37:24.193978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Date Parsing","metadata":{}},{"cell_type":"code","source":"df = train.copy() \ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.195502Z","iopub.execute_input":"2025-01-02T14:37:24.195803Z","iopub.status.idle":"2025-01-02T14:37:24.225365Z","shell.execute_reply.started":"2025-01-02T14:37:24.195774Z","shell.execute_reply":"2025-01-02T14:37:24.224766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['start_dates', 'end_dates']]\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.226011Z","iopub.execute_input":"2025-01-02T14:37:24.226203Z","iopub.status.idle":"2025-01-02T14:37:24.237318Z","shell.execute_reply.started":"2025-01-02T14:37:24.226187Z","shell.execute_reply":"2025-01-02T14:37:24.236542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_date(date_str: str):\n   if not isinstance(date_str, str):\n       return None \n       \n   # Clean input\n   date_str = date_str.strip().lower()\n   \n   # Handle special cases\n   special_cases = {'current', 'present', 'ongoing', 'till date', 'till date', '∞', 'n/a'}\n   if date_str in special_cases:\n       return (6, 2022)\n       \n   # Rest of the function remains same as before\n   # Handle cases ending with XX or xx\n   if date_str.endswith('xx'):\n       return None\n       \n   # Dictionary for month names\n   month_names = {\n       'jan': 1, 'january': 1,\n       'feb': 2, 'february': 2,\n       'mar': 3, 'march': 3,\n       'apr': 4, 'april': 4,\n       'may': 5,\n       'jun': 6, 'june': 6,\n       'jul': 7, 'july': 7,\n       'aug': 8, 'august': 8,\n       'sep': 9, 'sept': 9, 'september': 9,\n       'oct': 10, 'october': 10,\n       'nov': 11, 'november': 11,\n       'dec': 12, 'december': 12\n   }\n   \n   # Patterns for different date formats\n   patterns = [\n       # MM/YYYY or M/YYYY\n       r'^(\\d{1,2})/(\\d{4})$',\n       # Month YYYY or Month. YYYY\n       r'^([a-z]+\\.?\\s+)(\\d{4})$',\n       # Month DD, YYYY\n       r'^([a-z]+\\.?\\s+)\\d{1,2},\\s*(\\d{4})$',\n       # YYYY only\n       r'^(\\d{4})$',\n       # Season YYYY\n       r'^(spring|summer|fall|winter)\\s+(\\d{4})$'\n   ]\n   \n   # Try each pattern\n   for pattern in patterns:\n       match = re.match(pattern, date_str)\n       if match:\n           groups = match.groups()\n           \n           # Handle MM/YYYY format\n           if '/' in date_str:\n               month = int(groups[0])\n               year = int(groups[1])\n               return (month, year) if 1 <= month <= 12 else None\n           \n           # Handle year-only format\n           elif len(groups) == 1:\n               year = int(groups[0])\n               return None if year < 1900 else (1, year)\n           \n           # Handle month name formats\n           else:\n               month_str = groups[0].strip('. ').lower()\n               year = int(groups[1])\n               \n               if month_str in month_names:\n                   return (month_names[month_str], year)\n                   \n   return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.238041Z","iopub.execute_input":"2025-01-02T14:37:24.238311Z","iopub.status.idle":"2025-01-02T14:37:24.247032Z","shell.execute_reply.started":"2025-01-02T14:37:24.238290Z","shell.execute_reply":"2025-01-02T14:37:24.246315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"date_cols = ['start_dates', 'end_dates']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.247806Z","iopub.execute_input":"2025-01-02T14:37:24.248076Z","iopub.status.idle":"2025-01-02T14:37:24.260816Z","shell.execute_reply.started":"2025-01-02T14:37:24.248048Z","shell.execute_reply":"2025-01-02T14:37:24.259983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_experience_duration(row):\n    # Safely evaluate string representations of lists\n    start_dates = eval(row['start_dates']) if isinstance(row['start_dates'], str) else row['start_dates']\n    end_dates = eval(row['end_dates']) if isinstance(row['end_dates'], str) else row['end_dates']\n    \n    total_months = 0\n\n    if not (isinstance(start_dates, list) and isinstance(end_dates, list)):\n        return 0\n    \n    # Process each pair of start and end dates\n    for start_str, end_str in zip(start_dates, end_dates):\n        start_tuple = parse_date(start_str)\n        end_tuple = parse_date(end_str)\n        \n        if start_tuple and end_tuple:\n            start_month, start_year = start_tuple\n            end_month, end_year = end_tuple\n            \n            # Calculate months difference\n            months = (end_year - start_year) * 12 + (end_month - start_month)\n            if months > 0:\n                total_months += months\n    \n    return total_months\n\n# Example usage:\n# Assuming you have a DataFrame with 'start_dates' and 'end_dates' columns\ndef process_experience_data(df):\n    # for c in date_cols:\n    #     df[c] = df[c].apply(lambda x: f(x))\n    \n    # Create a copy to avoid modifying the original DataFrame\n    df_copy = df.copy()\n    \n    # Calculate duration for each row\n    df_copy['duration_months'] = df_copy.apply(calculate_experience_duration, axis=1)\n    \n    # Optionally add years column\n    df_copy['duration_years'] = df_copy['duration_months'] / 12\n    \n    return df_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.261631Z","iopub.execute_input":"2025-01-02T14:37:24.261864Z","iopub.status.idle":"2025-01-02T14:37:24.274122Z","shell.execute_reply.started":"2025-01-02T14:37:24.261845Z","shell.execute_reply":"2025-01-02T14:37:24.273286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"date_numeric_cols = ['duration_months', 'duration_years']\nnumeric_cols += date_numeric_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.274868Z","iopub.execute_input":"2025-01-02T14:37:24.275061Z","iopub.status.idle":"2025-01-02T14:37:24.287807Z","shell.execute_reply.started":"2025-01-02T14:37:24.275035Z","shell.execute_reply":"2025-01-02T14:37:24.287124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = train.copy()\ndf = df[:]\ndf = process_experience_data(df)[date_cols + date_numeric_cols]\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.288479Z","iopub.execute_input":"2025-01-02T14:37:24.288682Z","iopub.status.idle":"2025-01-02T14:37:24.743236Z","shell.execute_reply.started":"2025-01-02T14:37:24.288666Z","shell.execute_reply":"2025-01-02T14:37:24.742446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Age, Experience Requirements Parsing","metadata":{}},{"cell_type":"code","source":"def parse_requirements(df):\n    \"\"\"\n    Parse experience and age requirements to create min/max columns.\n    Returns DataFrame with 4 new columns:\n    - experience_min_years\n    - experience_max_years\n    - age_min_years\n    - age_max_years\n    \"\"\"\n    import re\n    \n    def parse_experience(text):\n        if pd.isna(text):\n            return None, None\n            \n        # Convert to string to ensure string operations work\n        text = str(text).lower().strip()\n        \n        # Handle \"At least X year(s)\" format\n        if \"at least\" in text:\n            years = re.findall(r'(\\d+)', text)[0]\n            return float(years), None\n            \n        # Handle \"X to Y years\" format\n        elif \"to\" in text:\n            numbers = re.findall(r'(\\d+)', text)\n            return float(numbers[0]), float(numbers[1])\n            \n        return None, None\n\n    def parse_age(text):\n        if pd.isna(text):\n            return None, None\n            \n        text = str(text).lower().strip()\n        \n        # Handle \"Age at least X years\" format\n        if \"at least\" in text:\n            years = re.findall(r'(\\d+)', text)[0]\n            return float(years), None\n            \n        # Handle \"Age at most X years\" format\n        elif \"at most\" in text:\n            years = re.findall(r'(\\d+)', text)[0]\n            return None, float(years)\n            \n        # Handle \"Age X to Y years\" format\n        elif \"to\" in text:\n            numbers = re.findall(r'(\\d+)', text)\n            return float(numbers[0]), float(numbers[1])\n            \n        return None, None\n\n    # Create new columns for experience requirements\n    df['experience_min_years'], df['experience_max_years'] = zip(*df['experiencere_requirement'].apply(parse_experience))\n    df['experience_min_months'] = df['experience_min_years']*12\n    df['experience_max_months'] = df['experience_max_years']*12\n    \n    # Create new columns for age requirements\n    df['age_min_years'], df['age_max_years'] = zip(*df['age_requirement'].apply(parse_age))\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.744065Z","iopub.execute_input":"2025-01-02T14:37:24.744277Z","iopub.status.idle":"2025-01-02T14:37:24.753089Z","shell.execute_reply.started":"2025-01-02T14:37:24.744258Z","shell.execute_reply":"2025-01-02T14:37:24.752261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"req_numeric_cols = ['experience_min_years', 'experience_max_years', 'experience_min_months', 'experience_max_months', 'age_min_years', 'age_max_years']\nnumeric_cols += req_numeric_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.753861Z","iopub.execute_input":"2025-01-02T14:37:24.754063Z","iopub.status.idle":"2025-01-02T14:37:24.769503Z","shell.execute_reply.started":"2025-01-02T14:37:24.754045Z","shell.execute_reply":"2025-01-02T14:37:24.768563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = train.copy()\ndf = df[:3]\ndf = parse_requirements(df)[req_numeric_cols]\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.770370Z","iopub.execute_input":"2025-01-02T14:37:24.770585Z","iopub.status.idle":"2025-01-02T14:37:24.795173Z","shell.execute_reply.started":"2025-01-02T14:37:24.770567Z","shell.execute_reply":"2025-01-02T14:37:24.794393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Passing Year Parsing","metadata":{}},{"cell_type":"code","source":"def extract_max_passing_year(df):\n    \"\"\"\n    Extract the maximum passing year from the passing_years column and add it as a new column.\n    \n    Parameters:\n    df (pandas.DataFrame): Input DataFrame with a 'passing_years' column\n    \n    Returns:\n    pandas.DataFrame: DataFrame with new 'passing_year_max' column\n    \"\"\"\n    def parse_years(value):\n        # Handle None or NaN values\n        if pd.isna(value):\n            return None\n            \n        try:\n            # If the value is already a string representation of a number\n            if isinstance(value, (int, float)):\n                return int(value)\n                \n            # Handle string representations of lists or single values\n            if isinstance(value, str):\n                # Remove any quotes and brackets\n                cleaned = value.replace(\"'\", \"\").replace('\"', \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n                # Split on comma if present\n                years = [int(year.strip()) for year in cleaned.split(',') if year.strip().isdigit()]\n                return max(years) if years else None\n                \n            # Handle actual lists\n            if isinstance(value, (list, tuple)):\n                years = [int(year) for year in value if str(year).isdigit()]\n                return max(years) if years else None\n                \n        except (ValueError, TypeError):\n            return None\n            \n        return None\n    \n    # Create a copy of the DataFrame to avoid modifying the original\n    result_df = df.copy()\n    \n    # Add the new column with extracted maximum years\n    result_df['passing_year_max'] = result_df['passing_years'].apply(parse_years)\n\n    result_df['passing_year_max'] = result_df['passing_year_max'] - 2000\n    \n    return result_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.795851Z","iopub.execute_input":"2025-01-02T14:37:24.796048Z","iopub.status.idle":"2025-01-02T14:37:24.801831Z","shell.execute_reply.started":"2025-01-02T14:37:24.796031Z","shell.execute_reply":"2025-01-02T14:37:24.801058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"year_numeric_cols = ['passing_year_max']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.802568Z","iopub.execute_input":"2025-01-02T14:37:24.802850Z","iopub.status.idle":"2025-01-02T14:37:24.817336Z","shell.execute_reply.started":"2025-01-02T14:37:24.802831Z","shell.execute_reply":"2025-01-02T14:37:24.816692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = train.copy()\ndf = df[:3]\ndf = extract_max_passing_year(df)[year_numeric_cols]\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.820566Z","iopub.execute_input":"2025-01-02T14:37:24.820838Z","iopub.status.idle":"2025-01-02T14:37:24.835545Z","shell.execute_reply.started":"2025-01-02T14:37:24.820810Z","shell.execute_reply":"2025-01-02T14:37:24.834960Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Total Numeric Columns","metadata":{}},{"cell_type":"code","source":"numeric_cols = date_numeric_cols + req_numeric_cols + year_numeric_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.836817Z","iopub.execute_input":"2025-01-02T14:37:24.837013Z","iopub.status.idle":"2025-01-02T14:37:24.847583Z","shell.execute_reply.started":"2025-01-02T14:37:24.836996Z","shell.execute_reply":"2025-01-02T14:37:24.846769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_numeric_cols(df):\n    df = process_experience_data(df)\n    df = parse_requirements(df)\n    df = extract_max_passing_year(df)\n    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.848382Z","iopub.execute_input":"2025-01-02T14:37:24.848619Z","iopub.status.idle":"2025-01-02T14:37:24.860098Z","shell.execute_reply.started":"2025-01-02T14:37:24.848600Z","shell.execute_reply":"2025-01-02T14:37:24.859428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = train.copy() \ndf = prepare_numeric_cols(df)\ndf.head(3)[numeric_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:24.860886Z","iopub.execute_input":"2025-01-02T14:37:24.861133Z","iopub.status.idle":"2025-01-02T14:37:25.357317Z","shell.execute_reply.started":"2025-01-02T14:37:24.861114Z","shell.execute_reply":"2025-01-02T14:37:25.356341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Embedders","metadata":{}},{"cell_type":"code","source":"MAX_FEATURES = 224\nVECTOR_SIZE = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:25.358258Z","iopub.execute_input":"2025-01-02T14:37:25.358535Z","iopub.status.idle":"2025-01-02T14:37:25.362061Z","shell.execute_reply.started":"2025-01-02T14:37:25.358516Z","shell.execute_reply":"2025-01-02T14:37:25.361051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_embed = [\n        'educationaL_requirements',\n        '﻿job_position_name',\n        'responsibilities',\n        'skills_required',\n        'degree_names',\n        'major_field_of_studies',\n        'positions',\n        'related_skils_in_job',\n        'skills',\n        'career_objective',\n        'professional_company_names',\n        'experiencere_requirement'\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:25.363068Z","iopub.execute_input":"2025-01-02T14:37:25.363451Z","iopub.status.idle":"2025-01-02T14:37:25.375912Z","shell.execute_reply.started":"2025-01-02T14:37:25.363420Z","shell.execute_reply":"2025-01-02T14:37:25.375219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Group Embedders","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport numpy as np\nimport pandas as pd\nimport ast\n\nclass GroupHybridEmbedder:\n    def __init__(self, max_features=MAX_FEATURES, vector_size=VECTOR_SIZE, window=5, min_count=1, k1=1.5, b=0.75):\n        self.max_features = max_features\n        self.vector_size = vector_size\n        self.window = window\n        self.min_count = min_count\n        self.k1 = k1\n        self.b = b\n        \n        # Initialize embedders for each group\n        self.w2v_models = {}\n        self.tfidf_vectors = {}\n        self.hash_vectors = {}\n        self.count_vectors = {}\n        self.svd_models = {}\n        self.avdl = {}\n        self.is_fitted = {}\n        \n        # Define column groups\n        self.column_groups = {\n            'skills': [\n                'skills_required',\n                'related_skils_in_job',\n                'skills',\n                'responsibilities'\n            ],\n            'education': [\n                'educationaL_requirements',\n                'degree_names',\n                'major_field_of_studies'\n            ],\n            'position': [\n                '﻿job_position_name',\n                'positions',\n                'career_objective'\n            ],\n            'experience': [\n                'professional_company_names',\n                'experiencere_requirement'\n            ]\n        }\n        \n        # Reverse mapping from column to group\n        self.column_to_group = {\n            col: group for group, cols in self.column_groups.items() \n            for col in cols\n        }\n    \n    def preprocess_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower().strip()\n        text = text.replace('\"', ' ').replace(\"'\", ' ')\n        text = text.replace('- ', ' ').replace('• ', ' ')\n        return text\n    \n    def preprocess_for_w2v(self, text):\n        return self.preprocess_text(text).split()\n        \n    def preprocess_list(self, text):\n        if pd.isna(text):\n            return ''\n        try:\n            items = ast.literal_eval(text) if isinstance(text, str) else text\n            if isinstance(items, list):\n                return ' '.join(str(item) for item in items if item)\n        except:\n            return str(text)\n        return str(text)\n\n    def _combine_group_texts(self, df, group_columns):\n        \"\"\"Combine texts from all columns in a group into a single corpus\"\"\"\n        all_texts = []\n        for column in group_columns:\n            texts = [\n                self.preprocess_text(t) if column in ['experiencere_requirement', 'career_objective', '﻿job_position_name']\n                else self.preprocess_list(t) for t in df[column]\n            ]\n            all_texts.extend(texts)\n        return all_texts\n\n    def fit(self, df):\n        \"\"\"Fit embedders for each group using combined texts from all columns in the group\"\"\"\n        for group, columns in self.column_groups.items():\n            print(f\"Fitting group: {group}\")\n            try:\n                processed_texts = self._combine_group_texts(df, columns)\n                w2v_texts = [self.preprocess_for_w2v(t) for t in processed_texts]\n                \n                self.w2v_models[group] = Word2Vec(\n                    sentences=w2v_texts,\n                    vector_size=self.vector_size,\n                    window=self.window,\n                    min_count=self.min_count,\n                    workers=4\n                )\n                \n                self.tfidf_vectors[group] = TfidfVectorizer(\n                    max_features=self.max_features,\n                    ngram_range=(1, 3),\n                    token_pattern=r'(?u)\\b\\w[\\w-]*\\w\\b',\n                    min_df=1\n                )\n                tfidf_matrix = self.tfidf_vectors[group].fit_transform(processed_texts)\n                \n                self.hash_vectors[group] = HashingVectorizer(\n                    n_features=self.max_features,\n                    ngram_range=(1, 2),\n                    alternate_sign=False\n                )\n                \n                self.count_vectors[group] = CountVectorizer(\n                    max_features=self.max_features,\n                    ngram_range=(1, 2)\n                )\n                count_matrix = self.count_vectors[group].fit_transform(processed_texts)\n                \n                n_features = count_matrix.shape[1]\n                n_components = min(50, n_features - 1)\n                \n                if n_components > 0:\n                    self.svd_models[group] = TruncatedSVD(n_components=n_components)\n                    self.svd_models[group].fit(count_matrix)\n                else:\n                    self.svd_models[group] = None\n                \n                self.avdl[group] = count_matrix.sum(1).mean()\n                self.is_fitted[group] = True\n                \n            except Exception as e:\n                print(f\"Error fitting group {group}: {str(e)}\")\n                continue\n        return self\n\n    def transform(self, texts, column_name, embedding_types=None):\n        \"\"\"\n        Transform texts using selected embedding types\n        \n        Args:\n            texts: Input texts to transform\n            column_name: Name of the column being transformed\n            embedding_types: List of embedding types to use. Options:\n                - 'w2v': Word2Vec embeddings\n                - 'tfidf': TF-IDF features\n                - 'hash': Hash features\n                - 'svd': SVD features\n                - 'bm25': BM25 features\n                - 'pool': Pooled features\n                If None, uses all embedding types\n        \"\"\"\n        group = self.column_to_group[column_name]\n        if not self.is_fitted.get(group):\n            raise ValueError(f\"Models for group {group} must be fitted first\")\n            \n        all_types = ['w2v', 'tfidf', 'hash', 'svd', 'bm25', 'pool']\n        embedding_types = embedding_types or all_types\n        \n        processed_texts = [\n            self.preprocess_text(t) if column_name in ['experiencere_requirement', 'career_objective', '﻿job_position_name']\n            else self.preprocess_list(t) for t in texts\n        ]\n        \n        embeddings_list = []\n        \n        if 'w2v' in embedding_types:\n            w2v_texts = [self.preprocess_for_w2v(t) for t in processed_texts]\n            w2v_embeddings = []\n            for words in w2v_texts:\n                word_vectors = [\n                    self.w2v_models[group].wv[word]\n                    for word in words\n                    if word in self.w2v_models[group].wv\n                ]\n                embedding = np.mean(word_vectors, axis=0) if word_vectors else np.zeros(self.vector_size)\n                w2v_embeddings.append(embedding)\n            embeddings_list.append(np.array(w2v_embeddings))\n            \n        if 'tfidf' in embedding_types:\n            tfidf_matrix = self.tfidf_vectors[group].transform(processed_texts).toarray()\n            embeddings_list.append(tfidf_matrix)\n            \n        if 'hash' in embedding_types:\n            hash_matrix = self.hash_vectors[group].transform(processed_texts).toarray()\n            embeddings_list.append(hash_matrix)\n            \n        if 'svd' in embedding_types:\n            count_matrix = self.count_vectors[group].transform(processed_texts)\n            if self.svd_models[group] is not None:\n                svd_matrix = self.svd_models[group].transform(count_matrix)\n                embeddings_list.append(svd_matrix)\n            \n        if 'bm25' in embedding_types:\n            try:\n                count_matrix = self.count_vectors[group].transform(processed_texts)\n                dl = count_matrix.sum(1).A1\n                tf = count_matrix.toarray()\n                n_docs = len(processed_texts)\n                doc_freqs = np.asarray((count_matrix > 0).sum(0)).ravel()\n                idf = np.log((n_docs - doc_freqs + 0.5) / (doc_freqs + 0.5))\n                numerator = tf * (self.k1 + 1)\n                denominator = tf + self.k1 * (1 - self.b + self.b * dl[:, np.newaxis] / self.avdl[group])\n                bm25_matrix = (numerator / denominator) * idf\n                embeddings_list.append(bm25_matrix)\n            except Exception as e:\n                print(f\"Warning: BM25 calculation failed for group {group}: {str(e)}\")\n                \n        if 'pool' in embedding_types and 'tfidf' in embedding_types:\n            mean_pool = np.mean(tfidf_matrix, axis=1)\n            max_pool = np.max(tfidf_matrix, axis=1)\n            embeddings_list.append(np.column_stack([mean_pool, max_pool]))\n            \n        return np.hstack(embeddings_list) if embeddings_list else np.array([])\n\n# Helper functions\ndef fit_group_embedders(train_df):\n    embedder = GroupHybridEmbedder()\n    embedder.fit(train_df)\n    return embedder\n\ndef get_group_embeddings_df(df, embedder, column, embedding_types=None):\n    embeddings = embedder.transform(df[column], column, embedding_types)\n    return pd.DataFrame(\n        {f'{column}_embeddings{i}': embeddings[:, i] for i in range(embeddings.shape[1])},\n        index=df.index\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:25.376661Z","iopub.execute_input":"2025-01-02T14:37:25.376919Z","iopub.status.idle":"2025-01-02T14:37:25.398311Z","shell.execute_reply.started":"2025-01-02T14:37:25.376900Z","shell.execute_reply":"2025-01-02T14:37:25.397367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fitted_embedder = fit_embedders(train)\nfitted_embedder = fit_group_embedders(train)\n# fitted_embedder = fit_group_embedders(train, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:25.399085Z","iopub.execute_input":"2025-01-02T14:37:25.399276Z","iopub.status.idle":"2025-01-02T14:37:35.990669Z","shell.execute_reply.started":"2025-01-02T14:37:25.399260Z","shell.execute_reply":"2025-01-02T14:37:35.989044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(df, embedding_types=None):\n    df = prepare_numeric_cols(df) \n    \n    for column in columns_to_embed:\n        # train_embeddings = get_embeddings_df(df, fitted_embedder, column)\n        train_embeddings = get_group_embeddings_df(df, fitted_embedder, column, embedding_types)\n        df = pd.concat([df, train_embeddings], axis=1)\n        \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:35.991309Z","iopub.execute_input":"2025-01-02T14:37:35.991554Z","iopub.status.idle":"2025-01-02T14:37:35.999773Z","shell.execute_reply.started":"2025-01-02T14:37:35.991531Z","shell.execute_reply":"2025-01-02T14:37:35.998527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# embedding_types = [\"w2v\"]\n\n# train_df = preprocess(train, embedding_types)\n# test_df = preprocess(test, embedding_types)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.000317Z","iopub.execute_input":"2025-01-02T14:37:36.000589Z","iopub.status.idle":"2025-01-02T14:37:36.028089Z","shell.execute_reply.started":"2025-01-02T14:37:36.000558Z","shell.execute_reply":"2025-01-02T14:37:36.026697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.028765Z","iopub.execute_input":"2025-01-02T14:37:36.028995Z","iopub.status.idle":"2025-01-02T14:37:36.038936Z","shell.execute_reply.started":"2025-01-02T14:37:36.028973Z","shell.execute_reply":"2025-01-02T14:37:36.037731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KFOLD_N_SPLITS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.039434Z","iopub.execute_input":"2025-01-02T14:37:36.039665Z","iopub.status.idle":"2025-01-02T14:37:36.056972Z","shell.execute_reply.started":"2025-01-02T14:37:36.039644Z","shell.execute_reply":"2025-01-02T14:37:36.055429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NN Architecture ","metadata":{}},{"cell_type":"code","source":"from typing import List, Union, Optional\n\ndef generate_architecture(input_dim: int, num_layers: int, initial_width: Optional[int] = None) -> List[int]:\n     # If initial width not specified, use next power of 2 >= input_dim\n    if initial_width is None:\n        initial_width = 2 ** (input_dim - 1).bit_length()\n        # Round up to nearest multiple of 32 for better GPU utilization\n        initial_width = ((initial_width + 31) // 32) * 32\n        initial_width = min(512, initial_width)  # Cap at 512\n    \n    hidden_dims = []\n    current_dim = initial_width\n    \n    # Calculate how many times to halve the dimension\n    # Reserve last few layers for small dimensions (32, 16, 8)\n    num_halving_layers = max(0, num_layers - 3)\n    halving_frequency = max(1, num_halving_layers // 3)  # Halve dimension every n layers\n    \n    # Generate main network body\n    for i in range(num_layers - 3):\n        if i > 0 and i % halving_frequency == 0:\n            current_dim = current_dim // 2\n        hidden_dims.append(current_dim)\n    \n    # Add final layers with standard small dimensions\n    if num_layers >= 3:\n        hidden_dims.extend([32, 16, 8])\n    elif num_layers == 2:\n        hidden_dims.extend([16, 8])\n    elif num_layers == 1:\n        hidden_dims.append(8)\n    \n    return hidden_dims\n\nclass FlexibleNN(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        num_layers: int,\n        initial_width: Optional[int] = None,\n        activation: str = 'relu',\n        dropout_rate: float = 0.5,\n        use_batch_norm: bool = True,\n        use_residual: bool = True\n    ):\n        super(FlexibleNN, self).__init__()\n        \n        # Generate hidden dimensions\n        hidden_dims = generate_architecture(input_dim, num_layers, initial_width)\n        \n        # Store configuration\n        self.use_batch_norm = use_batch_norm\n        self.use_residual = use_residual\n        \n        # Set up activation function\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'leaky_relu':\n            self.activation = nn.LeakyReLU(negative_slope=0.01)\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        else:\n            raise ValueError(f\"Unsupported activation: {activation}\")\n        \n        # Create layers\n        self.layers = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n        self.residual_layers = nn.ModuleList()\n        \n        # Add input layer\n        all_dims = [input_dim] + hidden_dims + [1]  # Add output dimension\n        \n        # Create the network architecture\n        for i in range(len(all_dims) - 1):\n            # Main layer\n            self.layers.append(nn.Linear(all_dims[i], all_dims[i + 1]))\n            \n            # Batch normalization (except for the output layer)\n            if use_batch_norm and i < len(all_dims) - 2:\n                self.batch_norms.append(nn.BatchNorm1d(all_dims[i + 1]))\n            \n            # Residual connection (except for the output layer)\n            if use_residual and i < len(all_dims) - 2:\n                self.residual_layers.append(nn.Linear(all_dims[i], all_dims[i + 1]))\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for i in range(len(self.layers)):\n            is_last_layer = i == len(self.layers) - 1\n            \n            if self.use_residual and not is_last_layer:\n                residual = self.residual_layers[i](x)\n            \n            x = self.layers[i](x)\n            \n            if not is_last_layer:\n                if self.use_batch_norm:\n                    x = self.batch_norms[i](x)\n                x = self.activation(x)\n                if self.dropout is not None:\n                    x = self.dropout(x)\n                if self.use_residual:\n                    x = x + residual\n            \n        return x\n\n# Example usage:\n# small_net = FlexibleNN(input_dim=10, num_layers=6)  # Similar to original NNModel\n# medium_net = FlexibleNN(input_dim=10, num_layers=7, initial_width=512)  # Similar to BNNModel\n# large_net = FlexibleNN(input_dim=10, num_layers=12, initial_width=512)  # Similar to BigNNModel\n\n# Custom network with automatic width determination:\n# custom_net = FlexibleNN(\n#     input_dim=20,\n#     num_layers=5,  # Will automatically determine appropriate layer sizes\n#     activation='gelu',\n#     dropout_rate=0.3\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.057622Z","iopub.execute_input":"2025-01-02T14:37:36.059034Z","iopub.status.idle":"2025-01-02T14:37:36.091801Z","shell.execute_reply.started":"2025-01-02T14:37:36.059007Z","shell.execute_reply":"2025-01-02T14:37:36.090526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = [\n    'educationaL_requirements_embeddings',\n    '﻿job_position_name_embeddings',\n    'responsibilities_embeddings',\n    'skills_required_embeddings',\n    'degree_names_embeddings',\n    'major_field_of_studies_embeddings',\n    'positions_embeddings',\n    'related_skils_in_job_embeddings',\n    'skills_embeddings',\n    'career_objective_embeddings',\n    # 'professional_company_names_embeddings',\n    # 'experiencere_requirement_embeddings'\n]\nfeature_cols = []\n# for col in train_df.columns:\n#     ok = False\n#     for x in cols:\n#         if x in col:\n#             ok = True\n#     if ok:\n#         feature_cols.append(col)\n\n# feature_cols += numeric_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.093102Z","iopub.execute_input":"2025-01-02T14:37:36.093481Z","iopub.status.idle":"2025-01-02T14:37:36.107677Z","shell.execute_reply.started":"2025-01-02T14:37:36.093448Z","shell.execute_reply":"2025-01-02T14:37:36.106507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(feature_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.108866Z","iopub.execute_input":"2025-01-02T14:37:36.109176Z","iopub.status.idle":"2025-01-02T14:37:36.122257Z","shell.execute_reply.started":"2025-01-02T14:37:36.109141Z","shell.execute_reply":"2025-01-02T14:37:36.121405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'global': {\n        'kfold_n_splits': 5\n    },\n    'randomforest': {\n        'n_estimators': 100,           # Number of trees in the forest\n        'max_depth': None,             # Maximum depth of the trees (None for unlimited)\n        'min_samples_split': 2,        # Minimum samples required to split an internal node\n        'min_samples_leaf': 1,         # Minimum samples required to be at a leaf node\n        'max_features': 'sqrt',        # Number of features to consider when looking for the best split\n        'bootstrap': True,             # Whether bootstrap samples are used when building trees\n        'oob_score': True,            # Whether to use out-of-bag samples to estimate the generalization score\n        'warm_start': False,          # Whether to reuse the solution of the previous call to fit\n        'criterion': 'squared_error'   # The function to measure the quality of a split\n    },\n    'lightgbm': {\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.9,\n        'max_depth': 9,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'gpu_use_dp': True\n    },\n    'xgboost': {\n        'objective': 'reg:squarederror',\n        'learning_rate': 0.01,\n        'max_depth': 10,\n        'alpha': 0.1,\n        'lambda': 0.1,\n        'colsample_bytree': 0.9,\n        'tree_method': 'hist',  # Use the 'hist' method for histogram-based training\n        'device': 'cuda',  # Use GPU with CUDA\n        # 'predictor': 'gpu_predictor',  # Use GPU for prediction\n        'subsample': 0.8,  # Use 80% of the data to train each tree\n        'gamma': 0.1,  # Minimum loss reduction to make a further partition\n        'n_estimators': 1000,  # Number of boosting rounds\n        'scale_pos_weight': 1,  # For imbalanced classes\n        'min_child_weight': 1,  # Minimum sum of instance weight in a child\n        'max_bin': 256,  # Number of bins for histogram-based methods\n        'booster': 'gbtree',  # Use tree-based model\n        'max_leaves': 31,  # Maximum number of leaves in a tree\n        'num_parallel_tree': 1,  # Number of trees to grow in parallel\n        # 'verbose': 1\n    },\n    'catboost': {\n        'iterations': 100,\n        'learning_rate': 0.05,\n        'depth': 9,\n        'l2_leaf_reg': 3,\n        'random_seed': 42,\n        'loss_function': 'RMSE'\n    },\n    'neuralnetwork': {\n        'num_layers': 7,\n        'dropout_rate': 0.5,\n        'use_batch_norm': True,\n        'use_residual': True,\n        'learning_rate': 0.001,\n        'epochs': 1000,\n        'batch_size': 32\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.123237Z","iopub.execute_input":"2025-01-02T14:37:36.123675Z","iopub.status.idle":"2025-01-02T14:37:36.134164Z","shell.execute_reply.started":"2025-01-02T14:37:36.123640Z","shell.execute_reply":"2025-01-02T14:37:36.133197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model_type='lightgbm', comment=\"\"):\n    # Check if GPU is available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    kf = KFold(n_splits=params['global']['kfold_n_splits'], shuffle=True, random_state=42)\n    cv_scores = []\n    test_preds = np.zeros(len(test_df))\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        print(f\"Training fold {fold + 1}...\")\n        X_train, X_val = train_df.iloc[train_idx][feature_cols], train_df.iloc[val_idx][feature_cols]\n        y_train, y_val = train_df.iloc[train_idx]['matched_score'], train_df.iloc[val_idx]['matched_score']\n\n        if model_type == 'randomforest':\n            model = RandomForestRegressor(\n                **params['randomforest'],\n                n_jobs=-1,  # Use all available cores\n                random_state=42\n            )\n            model.fit(X_train, y_train)\n            val_preds = model.predict(X_val)\n        elif model_type == 'lightgbm':\n            train_data = lgbm.Dataset(X_train, label=y_train)\n            val_data = lgbm.Dataset(X_val, label=y_val)\n            model = lgbm.train(\n                params=params['lightgbm'],\n                train_set=train_data,\n                num_boost_round=1000,\n                valid_sets=[train_data, val_data],\n                callbacks=[lgbm.early_stopping(stopping_rounds=50)]\n            )\n            val_preds = model.predict(X_val)\n        elif model_type == 'xgboost':\n            model = xgb.XGBRegressor(**params['xgboost'],\n                                    early_stopping_rounds=50,\n                                    enable_categorical=False\n                                    )\n            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n            val_preds = model.predict(X_val)\n        elif model_type == 'catboost':\n            model = cb.CatBoostRegressor(\n                **params['catboost'],\n                early_stopping_rounds=20,\n                verbose=100\n            )\n            model.fit(\n                X_train, y_train,\n                eval_set=(X_val, y_val),\n                use_best_model=True,\n                verbose=False\n            )\n            val_preds = model.predict(X_val)\n        elif model_type == 'neuralnetwork':\n            # Prepare data for PyTorch\n            X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n            X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n            y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n            y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n\n            input_dim = X_train.shape[1]\n            # model = NNModel(input_dim).to(device)\n\n            num_layers = params['neuralnetwork']['num_layers']\n            dropout_rate = params['neuralnetwork']['dropout_rate']\n            use_batch_norm = params['neuralnetwork']['use_batch_norm']\n            use_residual = params['neuralnetwork']['use_residual']\n            \n            model = FlexibleNN(input_dim, \n                               num_layers=num_layers, \n                               dropout_rate=dropout_rate,\n                               use_batch_norm=use_batch_norm,\n                               use_residual=use_residual\n                              )\n            model.to(device)\n            \n            criterion = nn.MSELoss()\n            optimizer = optim.Adam(model.parameters(), lr=params['neuralnetwork']['learning_rate'])\n            # optimizer = optim.AdamW(model.parameters(), lr=params['neuralnetwork']['learning_rate'], weight_decay=1e-4)\n            \n            \n            # Training loop\n            model.train()\n            best_val_loss = float('inf')\n            for epoch in range(params['neuralnetwork']['epochs']):\n                optimizer.zero_grad()\n                outputs = model(X_train_tensor)\n                loss = criterion(outputs.squeeze(), y_train_tensor)\n                loss.backward()\n                optimizer.step()\n\n                if (epoch+1)%100 == 0:\n                    # Validation\n                    model.eval()\n                    with torch.no_grad():\n                        val_outputs = model(X_val_tensor)\n                        val_loss = criterion(val_outputs.squeeze(), y_val_tensor)\n                    \n                    print(f\"Epoch [{epoch + 1}/{params['neuralnetwork']['epochs']}], \"\n                          f\"Train Loss: {loss.item():.4f}, Validation Loss: {val_loss.item():.4f}\")\n                    if val_loss.item() < best_val_loss:\n                        best_val_loss = val_loss.item()\n                        best_model_state = model.state_dict()  # Save the best model state\n                    # else:\n                    #     break\n                    model.train()\n\n            print(f\"Best Loss: {best_val_loss}\")\n            model.load_state_dict(best_model_state)\n            # Validation\n            model.eval()\n            with torch.no_grad():\n                val_preds = model(X_val_tensor).cpu().numpy().squeeze()\n\n        fold_score = mean_squared_error(y_val, val_preds)\n        cv_scores.append(fold_score)\n        \n        # For Neural Network, predictions should be made by passing data through the model\n        if model_type == 'neuralnetwork':\n            test_preds += model(torch.tensor(test_df[feature_cols].values, dtype=torch.float32).to(device)).cpu().detach().numpy().squeeze() / kf.n_splits\n        else:\n            test_preds += model.predict(test_df[feature_cols]) / kf.n_splits\n        \n        print(f\"MSE: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n    filename=f\"submission_{comment}_{get_formatted_time_date()}_{model_type}_{np.mean(cv_scores):.6f}.csv\"    \n    pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_preds\n    }).to_csv(filename, index=False)\n    print(f\"Submission saved to {filename}\")\n    return np.mean(cv_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.135199Z","iopub.execute_input":"2025-01-02T14:37:36.136066Z","iopub.status.idle":"2025-01-02T14:37:36.161550Z","shell.execute_reply.started":"2025-01-02T14:37:36.136034Z","shell.execute_reply":"2025-01-02T14:37:36.160185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_types = ['w2v', 'tfidf', 'hash', 'svd', 'bm25', 'pool']\nembedding_types_list = [ [\"w2v\"], [\"tfidf\"], [\"hash\"], [\"svd\"], [\"bm25\"], [\"w2v\", \"tfidf\", \"hash\"], all_types ]\n\nfor embedding_types in embedding_types_list:\n    train_df = preprocess(train, embedding_types)\n    test_df = preprocess(test, embedding_types)\n\n    feature_cols = []\n\n    for col in train_df.columns:\n        ok = False\n        for x in cols:\n            if x in col:\n                ok = True\n        if ok:\n            feature_cols.append(col)\n    \n    feature_cols += numeric_cols\n\n    print(len(feature_cols))\n\n    train_model('neuralnetwork', comment=(\"_\".join(embedding_types)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:37:36.162589Z","iopub.execute_input":"2025-01-02T14:37:36.162944Z","iopub.status.idle":"2025-01-02T14:39:11.108863Z","shell.execute_reply.started":"2025-01-02T14:37:36.162913Z","shell.execute_reply":"2025-01-02T14:39:11.107383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for model_name in ['randomforest', 'lightgbm', 'xgboost', 'catboost', 'neuralnetwork']:\n#     train_model(model_name)\n    \n# train_model('randomforest')\n# train_model('lightgbm')\n# train_model('xgboost')\n# train_model('catboost')\n# train_model('neuralnetwork')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:39:11.109623Z","iopub.status.idle":"2025-01-02T14:39:11.110059Z","shell.execute_reply":"2025-01-02T14:39:11.109863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from copy import deepcopy\n\n# def parameter_sweep():\n#     # Initialize results list\n#     results = []\n    \n#     # Base parameters (current defaults)\n#     base_params = {\n#         'num_layers': 7,\n#         'dropout_rate': 0.5,\n#         'use_batch_norm': True,\n#         'use_residual': True,\n#         'learning_rate': 0.001,\n#         'epochs': 1000,\n#         'batch_size': 32\n#     }\n    \n#     # Parameter ranges to test\n#     param_ranges = {\n#         'num_layers': range(3, 15),\n#         'dropout_rate': np.arange(0, 1.0, 0.1),\n#         'use_batch_norm': [True, False],\n#         'use_residual': [True, False]\n#     }\n\n#     # param_ranges = {\n#     #     # 'num_layers': range(5, 6),\n#     #     # 'dropout_rate': np.arange(0, 0.9, 0.2),\n#     #     'use_batch_norm': [True, False],\n#     #     'use_residual': [True, False]\n#     # }\n    \n#     # Test each parameter independently\n#     for param_name, param_values in param_ranges.items():\n#         for value in param_values:\n#             # Create a copy of base parameters\n#             current_params = deepcopy(base_params)\n            \n#             # Update the current parameter\n#             current_params[param_name] = value\n            \n#             # Update the global params dictionary\n#             params['neuralnetwork'] = current_params\n            \n#             print(f\"\\nTesting {param_name}: {value}\")\n            \n#             # Train model and get validation score\n#             cv_score = train_model(model_type='neuralnetwork')\n            \n#             # Store results\n#             result = {\n#                 'parameter': param_name,\n#                 'value': str(value),  # Convert to string for consistent CSV storage\n#                 'cv_score': cv_score,\n#                 # 'config': json.dumps(current_params)\n#             }\n#             results.append(result)\n            \n#             # Save intermediate results\n#             # pd.DataFrame(results).to_csv('parameter_sweep_results.csv', index=False)\n            \n#     return results\n\n# # Run parameter sweep\n# results = parameter_sweep()\n# res_df = pd.DataFrame(results)\n# res_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:39:11.111198Z","iopub.status.idle":"2025-01-02T14:39:11.111574Z","shell.execute_reply":"2025-01-02T14:39:11.111451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# res_df.to_csv('parameter_sweep_results.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T14:39:11.112480Z","iopub.status.idle":"2025-01-02T14:39:11.112877Z","shell.execute_reply":"2025-01-02T14:39:11.112661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}